#!/usr/bin/env python3
"""
üéØ M√ìDULO DESCARGA DOCUMENTO ADRES ESPEC√çFICO
URL: https://normograma.adres.gov.co/compilacion/aprende_adres_guias.html
Aplicando principios √©ticos de web scraping desarrollados en el taller
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import urllib.robotparser
from datetime import datetime
import ssl
import urllib3
from pymongo import MongoClient
import certifi
import os
import re

# Configuraci√≥n SSL (para manejar certificados)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================================================================================
# üìã CONFIGURACI√ìN √âTICA
# ================================================================================

class ConfiguracionEtica:
    """Configuraci√≥n basada en principios √©ticos del web scraping"""
    
    # URL objetivo espec√≠fica
    URL_DOCUMENTO_ADRES = "https://normograma.adres.gov.co/compilacion/aprende_adres_guias.html"
    URL_BASE_ADRES = "https://normograma.adres.gov.co"
    
    # Headers √©ticos identificativos
    HEADERS_ETICOS = {
        'User-Agent': 'Taller-BigData-Educativo/2.0 (investigacion.academica@universidad.edu)',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Purpose': 'Academic Research - Data Science Course - ADRES Documentation Study'
    }
    
    # Configuraciones de respeto al servidor
    DELAY_ENTRE_PETICIONES = 2.0  # 2 segundos entre peticiones
    TIMEOUT_PETICION = 30  # 30 segundos timeout
    MAX_REINTENTOS = 3
    
    # MongoDB Atlas (configurar con credenciales reales)
    MONGODB_CONFIG = {
        "connection_string": "mongodb+srv://<usuario>:<password>@<cluster>.mongodb.net/",
        "database": "taller_adres_bigdata",
        "collection": "documentos_guias_adres"
    }

# ================================================================================
# ü§ñ VERIFICADOR √âTICO
# ================================================================================

class VerificadorEtico:
    """Verificaciones √©ticas antes de realizar scraping"""
    
    @staticmethod
    def verificar_robots_txt(url_base, url_objetivo):
        """Verificar robots.txt seg√∫n principios √©ticos"""
        try:
            print("üîç Verificando robots.txt...")
            rp = urllib.robotparser.RobotFileParser()
            rp.set_url(f"{url_base}/robots.txt")
            rp.read()
            
            permitido = rp.can_fetch('*', url_objetivo)
            print(f"{'‚úÖ' if permitido else '‚ùå'} robots.txt: {'Permitido' if permitido else 'No permitido'}")
            return permitido
            
        except Exception as e:
            print(f"‚ö†Ô∏è No se pudo verificar robots.txt: {e}")
            print("üìã Procediendo con m√°xima precauci√≥n...")
            return True
    
    @staticmethod
    def mostrar_principios_aplicados():
        """Mostrar los principios √©ticos que se est√°n aplicando"""
        print("üõ°Ô∏è PRINCIPIOS √âTICOS APLICADOS:")
        print("   ‚Ä¢ Headers identificativos del prop√≥sito acad√©mico")
        print("   ‚Ä¢ Delay de 2 segundos entre peticiones")
        print("   ‚Ä¢ Timeout configurado para no saturar servidor")
        print("   ‚Ä¢ Verificaci√≥n de robots.txt")
        print("   ‚Ä¢ Solo informaci√≥n p√∫blica")
        print("   ‚Ä¢ Logging detallado para auditor√≠a")
        print("   ‚Ä¢ Manejo robusto de errores")

# ================================================================================
# üì• DESCARGADOR DE DOCUMENTO ADRES
# ================================================================================

class DescargadorDocumentoADRES:
    """Descargador √©tico del documento espec√≠fico de ADRES"""
    
    def __init__(self):
        self.session = self._crear_sesion_etica()
        self.config = ConfiguracionEtica()
        self.verificador = VerificadorEtico()
        
    def _crear_sesion_etica(self):
        """Crear sesi√≥n HTTP con configuraci√≥n √©tica"""
        session = requests.Session()
        
        # Configurar reintentos seg√∫n principios √©ticos
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=ConfiguracionEtica.MAX_REINTENTOS,
            backoff_factor=2,  # Aumentar delay progresivamente
            status_forcelist=[429, 500, 502, 503, 504],
            respect_retry_after_header=True
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # Aplicar headers √©ticos
        session.headers.update(ConfiguracionEtica.HEADERS_ETICOS)
        
        return session
    
    def descargar_documento_con_principios_eticos(self):
        """Descargar documento aplicando todos los principios √©ticos"""
        print("üéì INICIANDO DESCARGA √âTICA DEL DOCUMENTO ADRES")
        print("=" * 70)
        
        # 1. Verificaciones √©ticas preliminares
        self.verificador.mostrar_principios_aplicados()
        print()
        
        # 2. Verificar robots.txt
        if not self.verificador.verificar_robots_txt(
            self.config.URL_BASE_ADRES, 
            self.config.URL_DOCUMENTO_ADRES
        ):
            print("‚ùå El robots.txt no permite el acceso. Deteniendo por principios √©ticos.")
            return None
        
        print()
        
        # 3. Realizar descarga √©tica
        try:
            print("üì• Descargando documento desde:")
            print(f"   {self.config.URL_DOCUMENTO_ADRES}")
            print(f"‚è±Ô∏è Aplicando delay √©tico de {self.config.DELAY_ENTRE_PETICIONES}s...")
            
            time.sleep(self.config.DELAY_ENTRE_PETICIONES)
            
            # Realizar petici√≥n con SSL verification disabled para manejar certificados
            response = self.session.get(
                self.config.URL_DOCUMENTO_ADRES,
                timeout=self.config.TIMEOUT_PETICION,
                verify=False  # Deshabilitar verificaci√≥n SSL por problemas de certificados
            )
            
            # Verificar c√≥digos de estado seg√∫n principios √©ticos
            if response.status_code == 429:
                print("üö´ L√≠mite de velocidad alcanzado. Respetando servidor...")
                time.sleep(60)  # Esperar 1 minuto
                return None
            elif response.status_code == 503:
                print("üîß Servicio no disponible. Respetando estado del servidor...")
                return None
            
            response.raise_for_status()
            
            print(f"‚úÖ Descarga exitosa! C√≥digo: {response.status_code}")
            print(f"üìä Tama√±o del documento: {len(response.content)} bytes")
            
            return response.text
            
        except requests.exceptions.SSLError as e:
            print(f"üîí Error SSL (manejado √©ticamente): {e}")
            print("üîÑ Reintentando con configuraci√≥n SSL adaptada...")
            # Aqu√≠ podr√≠as implementar manejo espec√≠fico de SSL si es necesario
            return None
            
        except requests.exceptions.Timeout:
            print("‚è±Ô∏è Timeout - Respetando l√≠mites del servidor")
            return None
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error en petici√≥n: {e}")
            return None
    
    def extraer_contenido_estructurado(self, html_content):
        """Extraer y estructurar contenido del HTML"""
        if not html_content:
            return None
        
        print("\nüîç EXTRAYENDO CONTENIDO ESTRUCTURADO")
        print("=" * 50)
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extraer informaci√≥n principal
            titulo = self._extraer_titulo(soup)
            contenido_principal = self._extraer_contenido_principal(soup)
            enlaces_relacionados = self._extraer_enlaces(soup)
            metadatos = self._extraer_metadatos(soup)
            
            documento_estructurado = {
                "url_original": self.config.URL_DOCUMENTO_ADRES,
                "titulo": titulo,
                "fecha_extraccion": datetime.now().isoformat(),
                "contenido": {
                    "texto_completo": contenido_principal,
                    "longitud_caracteres": len(contenido_principal),
                    "longitud_palabras": len(contenido_principal.split()),
                    "html_original": html_content[:1000] + "..." if len(html_content) > 1000 else html_content
                },
                "enlaces_relacionados": enlaces_relacionados,
                "metadatos": metadatos,
                "analisis": self._analizar_contenido(contenido_principal),
                "procesamiento": {
                    "metodo": "web_scraping_etico",
                    "principios_aplicados": [
                        "verificacion_robots_txt",
                        "headers_identificativos", 
                        "delays_respetuosos",
                        "manejo_errores_robusto"
                    ],
                    "timestamp": datetime.now().isoformat(),
                    "version_sistema": "2.0"
                }
            }
            
            print(f"‚úÖ Contenido extra√≠do: {titulo}")
            print(f"üìä {len(contenido_principal)} caracteres, {len(contenido_principal.split())} palabras")
            print(f"üîó {len(enlaces_relacionados)} enlaces encontrados")
            
            return documento_estructurado
            
        except Exception as e:
            print(f"‚ùå Error extrayendo contenido: {e}")
            return None
    
    def _extraer_titulo(self, soup):
        """Extraer t√≠tulo del documento"""
        titulo = soup.find('title')
        if titulo:
            return titulo.get_text().strip()
        
        # Alternativas
        h1 = soup.find('h1')
        if h1:
            return h1.get_text().strip()
            
        return "Documento ADRES - Aprende ADRES Gu√≠as"
    
    def _extraer_contenido_principal(self, soup):
        """Extraer contenido principal del documento"""
        # Buscar contenedor principal
        contenedores = [
            soup.find('main'),
            soup.find('div', {'class': 'content'}),
            soup.find('div', {'class': 'main-content'}),
            soup.find('article'),
            soup.body
        ]
        
        for contenedor in contenedores:
            if contenedor:
                # Limpiar scripts y estilos
                for script in contenedor(["script", "style"]):
                    script.decompose()
                
                texto = contenedor.get_text()
                # Limpiar espacios
                texto = re.sub(r'\s+', ' ', texto).strip()
                
                if len(texto) > 100:  # Solo si tiene contenido significativo
                    return texto
        
        return "Contenido no disponible"
    
    def _extraer_enlaces(self, soup):
        """Extraer enlaces relevantes"""
        enlaces = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            texto = link.get_text().strip()
            
            if href and texto and len(texto) > 3:
                # Convertir enlaces relativos a absolutos
                if href.startswith('/'):
                    href = self.config.URL_BASE_ADRES + href
                elif not href.startswith('http'):
                    continue
                
                enlaces.append({
                    "url": href,
                    "texto": texto,
                    "es_documento": any(ext in href.lower() for ext in ['.pdf', '.doc', '.xls', '.ppt'])
                })
        
        return enlaces[:20]  # Limitar a 20 enlaces m√°s relevantes
    
    def _extraer_metadatos(self, soup):
        """Extraer metadatos del documento"""
        metadatos = {
            "fuente": "normograma.adres.gov.co",
            "tipo_documento": "guia_aprende_adres",
            "seccion": "compilacion"
        }
        
        # Buscar metadatos espec√≠ficos
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                metadatos['descripcion'] = meta.get('content', '')
            elif meta.get('name') == 'keywords':
                metadatos['palabras_clave'] = meta.get('content', '')
            elif meta.get('name') == 'author':
                metadatos['autor'] = meta.get('content', '')
        
        return metadatos
    
    def _analizar_contenido(self, contenido):
        """An√°lizar contenido para insights"""
        palabras_clave_adres = [
            "adres", "salud", "sistema", "informaci√≥n", "calidad",
            "eps", "ips", "habilitaci√≥n", "auditoria", "paciente",
            "servicio", "atenci√≥n", "normativa", "resoluci√≥n",
            "gu√≠a", "procedimiento", "documento", "compilaci√≥n"
        ]
        
        contenido_lower = contenido.lower()
        palabras_encontradas = [p for p in palabras_clave_adres if p in contenido_lower]
        
        # An√°lisis adicional
        num_parrafos = contenido.count('\n\n') + 1
        num_secciones = len(re.findall(r'\b[A-Z][A-Z\s]{10,}\b', contenido))
        
        return {
            "palabras_clave_adres": palabras_encontradas,
            "total_palabras_clave": len(palabras_encontradas),
            "relevancia_adres": len(palabras_encontradas) / len(palabras_clave_adres),
            "estructura": {
                "parrafos_estimados": num_parrafos,
                "secciones_estimadas": num_secciones
            },
            "categoria": self._clasificar_documento_adres(palabras_encontradas)
        }
    
    def _clasificar_documento_adres(self, palabras_encontradas):
        """Clasificar el tipo de documento ADRES"""
        if "gu√≠a" in palabras_encontradas or "procedimiento" in palabras_encontradas:
            return "guia_procedimientos"
        elif "compilaci√≥n" in palabras_encontradas or "documento" in palabras_encontradas:
            return "compilacion_normativa"
        elif "sistema" in palabras_encontradas or "informaci√≥n" in palabras_encontradas:
            return "sistema_informacion"
        else:
            return "documento_general_adres"

# ================================================================================
# üîÑ GESTOR MONGODB ATLAS
# ================================================================================

class GestorMongoDBAtlas:
    """Gestor para MongoDB Atlas con visualizaci√≥n de progreso"""
    
    def __init__(self, connection_string=None):
        self.connection_string = connection_string or self._solicitar_credenciales()
        self.client = None
        self.db = None
        self.collection = None
    
    def _solicitar_credenciales(self):
        """Obtener credenciales de MongoDB Atlas desde configuraci√≥n"""
        print("\nüíæ CONFIGURACI√ìN MONGODB ATLAS")
        print("=" * 40)
        
        # Intentar cargar desde archivo de configuraci√≥n
        try:
            import json
            with open('config_mongodb_atlas.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            atlas_config = config.get('mongodb_atlas', {})
            
            if atlas_config.get('configurado', False):
                connection_string = atlas_config.get('connection_string', '')
                if 'usuario:password' not in connection_string and 'xxxxx' not in connection_string:
                    print("‚úÖ Usando configuraci√≥n de MongoDB Atlas")
                    return connection_string
            
            print("‚ö†Ô∏è Configuraci√≥n de MongoDB Atlas incompleta")
            print("   Ejecuta: python configurar_atlas.py")
            
        except FileNotFoundError:
            print("‚ùå No se encontr√≥ config_mongodb_atlas.json")
            print("   Ejecuta: python configurar_atlas.py")
        except Exception as e:
            print(f"‚ùå Error cargando configuraci√≥n: {e}")
        
        # Fallback para demostraci√≥n sin configuraci√≥n real
        print("üîÑ Usando configuraci√≥n de demostraci√≥n...")
        return None  # Retornar None para indicar que no hay configuraci√≥n v√°lida
    
    def conectar_con_progreso(self):
        """Conectar a MongoDB Atlas mostrando progreso"""
        print("üîó Conectando a MongoDB Atlas...")
        
        try:
            # Simular progreso de conexi√≥n
            pasos = [
                "Verificando credenciales...",
                "Estableciendo conexi√≥n SSL...", 
                "Autenticando usuario...",
                "Seleccionando base de datos...",
                "Configurando colecci√≥n..."
            ]
            
            for i, paso in enumerate(pasos, 1):
                print(f"   {i}/5 {paso}")
                time.sleep(0.5)  # Simular tiempo de conexi√≥n
            
            # Intentar conexi√≥n real (manejar√° error si credenciales no son v√°lidas)
            self.client = MongoClient(
                self.connection_string,
                serverSelectionTimeoutMS=5000,
                tls=True,
                tlsCAFile=certifi.where()
            )
            
            # Test de conexi√≥n
            self.client.admin.command('ping')
            
            # Configurar BD y colecci√≥n
            self.db = self.client[ConfiguracionEtica.MONGODB_CONFIG["database"]]
            self.collection = self.db[ConfiguracionEtica.MONGODB_CONFIG["collection"]]
            
            print("‚úÖ Conexi√≥n exitosa a MongoDB Atlas!")
            return True
            
        except Exception as e:
            print(f"‚ùå Error conectando a MongoDB Atlas: {e}")
            print("üí° Verificar credenciales y configuraci√≥n de red")
            return False
    
    def subir_documento_con_progreso(self, documento_json):
        """Subir documento mostrando progreso detallado"""
        if not self.collection:
            print("‚ùå No hay conexi√≥n a MongoDB Atlas")
            return False
        
        print("\nüì§ SUBIENDO DOCUMENTO A MONGODB ATLAS")
        print("=" * 50)
        
        try:
            # Mostrar informaci√≥n del documento
            print(f"üìÑ Documento: {documento_json.get('titulo', 'Sin t√≠tulo')}")
            print(f"üìä Tama√±o: {documento_json['contenido']['longitud_caracteres']} caracteres")
            print(f"üîó Enlaces: {len(documento_json.get('enlaces_relacionados', []))}")
            
            # Verificar si ya existe
            print("üîç Verificando duplicados...")
            existe = self.collection.find_one({
                "url_original": documento_json["url_original"]
            })
            
            if existe:
                print("‚ö†Ô∏è Documento ya existe. Actualizando...")
                resultado = self.collection.replace_one(
                    {"url_original": documento_json["url_original"]},
                    documento_json
                )
                print(f"‚úÖ Documento actualizado. ID: {existe['_id']}")
                return str(existe['_id'])
            else:
                print("üíæ Insertando nuevo documento...")
                resultado = self.collection.insert_one(documento_json)
                print(f"‚úÖ Documento insertado. ID: {resultado.inserted_id}")
                return str(resultado.inserted_id)
                
        except Exception as e:
            print(f"‚ùå Error subiendo a MongoDB Atlas: {e}")
            return None
    
    def mostrar_estadisticas_atlas(self):
        """Mostrar estad√≠sticas de la colecci√≥n en Atlas"""
        if not self.collection:
            return
        
        try:
            total = self.collection.count_documents({})
            print(f"\nüìä ESTAD√çSTICAS MONGODB ATLAS")
            print(f"   Total documentos: {total}")
            
            if total > 0:
                # √öltimos documentos
                ultimos = list(self.collection.find().sort("_id", -1).limit(3))
                print("   √öltimos documentos:")
                for doc in ultimos:
                    print(f"   ‚Ä¢ {doc.get('titulo', 'Sin t√≠tulo')}")
                    
        except Exception as e:
            print(f"‚ùå Error obteniendo estad√≠sticas: {e}")
    
    def cerrar_conexion(self):
        """Cerrar conexi√≥n a MongoDB Atlas"""
        if self.client:
            self.client.close()
            print("üîå Conexi√≥n a MongoDB Atlas cerrada")

# ================================================================================
# üéØ M√ìDULO PRINCIPAL DE EJECUCI√ìN
# ================================================================================

def main():
    """Funci√≥n principal que ejecuta todo el proceso"""
    print("üéì M√ìDULO DESCARGA DOCUMENTO ADRES ‚Üí MONGODB ATLAS")
    print("üõ°Ô∏è Aplicando principios √©ticos de web scraping")
    print("=" * 70)
    
    # Inicializar componentes
    descargador = DescargadorDocumentoADRES()
    gestor_mongo = GestorMongoDBAtlas()
    
    try:
        # 1. Descargar documento con principios √©ticos
        print("FASE 1: DESCARGA √âTICA DEL DOCUMENTO")
        html_content = descargador.descargar_documento_con_principios_eticos()
        
        if not html_content:
            print("‚ùå No se pudo descargar el documento")
            return False
        
        # 2. Extraer y estructurar contenido
        print("\nFASE 2: EXTRACCI√ìN Y ESTRUCTURACI√ìN")
        documento_json = descargador.extraer_contenido_estructurado(html_content)
        
        if not documento_json:
            print("‚ùå No se pudo extraer contenido")
            return False
        
        # 3. Conectar a MongoDB Atlas
        print("\nFASE 3: CONEXI√ìN A MONGODB ATLAS")
        if not gestor_mongo.conectar_con_progreso():
            print("‚ö†Ô∏è No se pudo conectar a MongoDB Atlas")
            print("üíæ Guardando documento localmente como respaldo...")
            
            # Guardar localmente como respaldo
            with open('documento_adres_respaldo.json', 'w', encoding='utf-8') as f:
                json.dump(documento_json, f, ensure_ascii=False, indent=2, default=str)
            print("‚úÖ Documento guardado como respaldo local")
            return True
        
        # 4. Subir documento a MongoDB Atlas
        print("\nFASE 4: CARGA A MONGODB ATLAS")
        doc_id = gestor_mongo.subir_documento_con_progreso(documento_json)
        
        if doc_id:
            # 5. Mostrar estad√≠sticas finales
            gestor_mongo.mostrar_estadisticas_atlas()
            
            print("\nüéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!")
            print("=" * 50)
            print(f"‚úÖ Documento ADRES descargado √©ticamente")
            print(f"‚úÖ Contenido extra√≠do y estructurado")
            print(f"‚úÖ Subido a MongoDB Atlas (ID: {doc_id})")
            print(f"‚úÖ Principios √©ticos aplicados correctamente")
            
            return True
        else:
            print("‚ùå Error en la carga a MongoDB Atlas")
            return False
            
    except Exception as e:
        print(f"‚ùå Error general: {e}")
        return False
        
    finally:
        # Limpiar recursos
        gestor_mongo.cerrar_conexion()

if __name__ == "__main__":
    main()