{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92df22d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéì Universidad:** Universidad Central de Colombia\n",
    "\n",
    "**üë®‚Äçüíª Autor:** Efren Bohorquez Vargas\n",
    "\n",
    "**üìö Curso:** Big Data\n",
    "\n",
    "**üë®‚Äçüè´ Presentado a:** Luis Fernando Castellanos\n",
    "\n",
    "**üìÖ Fecha:** Octubre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b645d",
   "metadata": {},
   "source": [
    "# üéì Taller Big Data: Web Scraping √âtico a ADRES\n",
    "\n",
    "**ADRES**: Administradora de los Recursos del Sistema General de Seguridad Social en Salud\n",
    "\n",
    "## üìã Objetivos del Taller\n",
    "\n",
    "1. Comprender los principios del **web scraping √©tico**\n",
    "2. Extraer documentos oficiales de ADRES\n",
    "3. Almacenar datos estructurados en **MongoDB Atlas**\n",
    "4. Analizar contenido de documentos legales\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881e26f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Instalaci√≥n de Dependencias\n",
    "\n",
    "Ejecutar solo si las librer√≠as no est√°n instaladas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar si necesitas instalar las dependencias\n",
    "# !pip install requests beautifulsoup4 pymongo urllib3 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b95a38",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13318964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb51b6",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuraci√≥n √âtica del Scraper\n",
    "\n",
    "### Principios √âticos:\n",
    "- ‚úÖ Respetar `robots.txt`\n",
    "- ‚úÖ Implementar delays entre requests (2+ segundos)\n",
    "- ‚úÖ User-Agent identificado\n",
    "- ‚úÖ Solo contenido p√∫blico\n",
    "- ‚úÖ Prop√≥sito educativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dc374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuraci√≥n √©tica establecida\n",
      "   ‚Ä¢ Delay: 3.0s\n",
      "   ‚Ä¢ User-Agent: TallerBigData-WebScraping/1.0 (Educativo; Python/requests)\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n √©tica del scraper\n",
    "CONFIGURACION_ETICA = {\n",
    "    'delay_entre_requests': 3.0,  # 3 segundos entre peticiones\n",
    "    'max_reintentos': 3,\n",
    "    'timeout': 30,\n",
    "    'user_agent': 'TallerBigData-WebScraping/1.0 (Educativo; Python/requests)',\n",
    "    'headers': {\n",
    "        'User-Agent': 'TallerBigData-WebScraping/1.0 (Educativo; Python/requests)',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'es-CO,es;q=0.9',\n",
    "        'Purpose': 'Educational Research - Big Data Workshop'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuraci√≥n √©tica establecida\")\n",
    "print(f\"   ‚Ä¢ Delay: {CONFIGURACION_ETICA['delay_entre_requests']}s\")\n",
    "print(f\"   ‚Ä¢ User-Agent: {CONFIGURACION_ETICA['user_agent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277547ee",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verificar robots.txt\n",
    "\n",
    "Siempre verificar qu√© permite el sitio web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b92ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No se encontr√≥ robots.txt (permitido por defecto)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verificar_robots_txt(url_base):\n",
    "    \"\"\"Verificar y mostrar robots.txt del sitio\"\"\"\n",
    "    robots_url = urljoin(url_base, '/robots.txt')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"ü§ñ robots.txt encontrado:\")\n",
    "            print(\"‚îÄ\" * 50)\n",
    "            print(response.text[:500])  # Primeras 500 caracteres\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se encontr√≥ robots.txt (permitido por defecto)\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error verificando robots.txt: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verificar robots.txt de ADRES\n",
    "URL_BASE_ADRES = \"https://www.adres.gov.co\"\n",
    "verificar_robots_txt(URL_BASE_ADRES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d64083",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Funci√≥n de Scraping √âtico\n",
    "\n",
    "Implementaci√≥n con delays y manejo de errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aea5378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de scraping √©tico definida\n"
     ]
    }
   ],
   "source": [
    "def scraping_etico(url, config=CONFIGURACION_ETICA):\n",
    "    \"\"\"\n",
    "    Realizar scraping √©tico con delays y reintentos\n",
    "    \n",
    "    Args:\n",
    "        url: URL a scrapear\n",
    "        config: Configuraci√≥n √©tica\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object o None si falla\n",
    "    \"\"\"\n",
    "    for intento in range(config['max_reintentos']):\n",
    "        try:\n",
    "            # Delay √©tico antes de hacer request\n",
    "            if intento > 0:\n",
    "                print(f\"   ‚è≥ Reintento {intento + 1}/{config['max_reintentos']}...\")\n",
    "            \n",
    "            time.sleep(config['delay_entre_requests'])\n",
    "            \n",
    "            # Realizar request\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers=config['headers'],\n",
    "                timeout=config['timeout']\n",
    "            )\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parsear HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            print(f\"‚úÖ Contenido extra√≠do exitosamente de: {url}\")\n",
    "            return soup\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error en intento {intento + 1}: {e}\")\n",
    "            if intento == config['max_reintentos'] - 1:\n",
    "                print(f\"‚ùå Fall√≥ despu√©s de {config['max_reintentos']} intentos\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de scraping √©tico definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7f0e6",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Extraer Enlaces a Documentos\n",
    "\n",
    "Buscar enlaces a PDFs y documentos oficiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38432f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de extracci√≥n de enlaces definida\n"
     ]
    }
   ],
   "source": [
    "def extraer_enlaces_documentos(soup, url_base):\n",
    "    \"\"\"\n",
    "    Extraer enlaces a documentos (PDFs, resoluciones, etc.)\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        url_base: URL base para construir URLs absolutas\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con informaci√≥n de documentos\n",
    "    \"\"\"\n",
    "    documentos = []\n",
    "    \n",
    "    # Buscar todos los enlaces\n",
    "    enlaces = soup.find_all('a', href=True)\n",
    "    \n",
    "    for enlace in enlaces:\n",
    "        href = enlace['href']\n",
    "        texto = enlace.get_text(strip=True)\n",
    "        \n",
    "        # Filtrar enlaces a documentos\n",
    "        if any(ext in href.lower() for ext in ['.pdf', 'resolucion', 'documento']):\n",
    "            url_completa = urljoin(url_base, href)\n",
    "            \n",
    "            documentos.append({\n",
    "                'url': url_completa,\n",
    "                'texto': texto,\n",
    "                'tipo': 'pdf' if '.pdf' in href.lower() else 'documento',\n",
    "                'fecha_extraccion': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    print(f\"üìÑ Encontrados {len(documentos)} enlaces a documentos\")\n",
    "    return documentos\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n de enlaces definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e0b42",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Analizar Contenido de P√°gina\n",
    "\n",
    "Extraer informaci√≥n estructurada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e5fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de an√°lisis de contenido definida\n"
     ]
    }
   ],
   "source": [
    "def analizar_contenido(soup, url):\n",
    "    \"\"\"\n",
    "    Analizar y extraer contenido estructurado\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        url: URL de origen\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con an√°lisis del contenido\n",
    "    \"\"\"\n",
    "    # Extraer t√≠tulo\n",
    "    titulo = soup.find('title')\n",
    "    titulo_texto = titulo.get_text(strip=True) if titulo else 'Sin t√≠tulo'\n",
    "    \n",
    "    # Extraer texto principal\n",
    "    texto_completo = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Palabras clave relacionadas con ADRES\n",
    "    palabras_clave = ['adres', 'salud', 'seguridad social', 'resoluci√≥n', \n",
    "                      'administradora', 'recursos', 'eps', 'ips']\n",
    "    \n",
    "    palabras_encontradas = {}\n",
    "    for palabra in palabras_clave:\n",
    "        cuenta = texto_completo.lower().count(palabra.lower())\n",
    "        if cuenta > 0:\n",
    "            palabras_encontradas[palabra] = cuenta\n",
    "    \n",
    "    # Extraer n√∫meros de resoluci√≥n (ejemplo: Resoluci√≥n 2876 de 2013)\n",
    "    resoluciones = re.findall(r'resoluci√≥n\\s+(\\d+)\\s+de\\s+(\\d{4})', \n",
    "                              texto_completo, re.IGNORECASE)\n",
    "    \n",
    "    analisis = {\n",
    "        'url': url,\n",
    "        'titulo': titulo_texto,\n",
    "        'longitud_texto': len(texto_completo),\n",
    "        'palabras_clave': palabras_encontradas,\n",
    "        'resoluciones_encontradas': resoluciones,\n",
    "        'fecha_analisis': datetime.now().isoformat(),\n",
    "        'relevancia_adres': len(palabras_encontradas) > 0\n",
    "    }\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de an√°lisis de contenido definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315181fc",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Conexi√≥n a MongoDB Atlas\n",
    "\n",
    "**Nota**: Reemplazar la cadena de conexi√≥n con tu propia configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8876f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de conexi√≥n a MongoDB definida\n"
     ]
    }
   ],
   "source": [
    "def conectar_mongodb(connection_string=None):\n",
    "    \"\"\"\n",
    "    Conectar a MongoDB Atlas\n",
    "    \n",
    "    Args:\n",
    "        connection_string: String de conexi√≥n de MongoDB Atlas\n",
    "    \n",
    "    Returns:\n",
    "        Tupla (client, database, collection)\n",
    "    \"\"\"\n",
    "    # IMPORTANTE: Reemplazar con tu propia connection string\n",
    "    if connection_string is None:\n",
    "        print(\"‚ö†Ô∏è Configurar connection_string de MongoDB Atlas\")\n",
    "        print(\"   Ejemplo: mongodb+srv://usuario:password@cluster.mongodb.net/\")\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        client = MongoClient(connection_string, serverSelectionTimeoutMS=5000)\n",
    "        \n",
    "        # Verificar conexi√≥n\n",
    "        client.admin.command('ping')\n",
    "        \n",
    "        db = client['taller_bigdata_adres']\n",
    "        collection = db['documentos_adres']\n",
    "        \n",
    "        print(\"‚úÖ Conectado a MongoDB Atlas\")\n",
    "        print(f\"   üìä Base de datos: {db.name}\")\n",
    "        print(f\"   üìÅ Colecci√≥n: {collection.name}\")\n",
    "        \n",
    "        return client, db, collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error conectando a MongoDB: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Descomentar y configurar tu connection string\n",
    "# MONGODB_URI = \"mongodb+srv://usuario:password@cluster.mongodb.net/\"\n",
    "# client, db, collection = conectar_mongodb(MONGODB_URI)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de conexi√≥n a MongoDB definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e80918",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Guardar en MongoDB\n",
    "\n",
    "Almacenar documentos extra√≠dos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271f9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de guardado en MongoDB definida\n"
     ]
    }
   ],
   "source": [
    "def guardar_en_mongodb(collection, documento):\n",
    "    \"\"\"\n",
    "    Guardar documento en MongoDB\n",
    "    \n",
    "    Args:\n",
    "        collection: Colecci√≥n de MongoDB\n",
    "        documento: Diccionario con datos a guardar\n",
    "    \n",
    "    Returns:\n",
    "        ID del documento insertado o None\n",
    "    \"\"\"\n",
    "    if collection is None:\n",
    "        print(\"‚ö†Ô∏è Colecci√≥n no configurada\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Verificar si ya existe (evitar duplicados)\n",
    "        existe = collection.find_one({'url': documento.get('url')})\n",
    "        \n",
    "        if existe:\n",
    "            print(f\"‚ÑπÔ∏è Documento ya existe: {documento.get('titulo', 'Sin t√≠tulo')}\")\n",
    "            return existe['_id']\n",
    "        \n",
    "        # Insertar nuevo documento\n",
    "        resultado = collection.insert_one(documento)\n",
    "        print(f\"‚úÖ Documento guardado: {documento.get('titulo', 'Sin t√≠tulo')}\")\n",
    "        \n",
    "        return resultado.inserted_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error guardando documento: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de guardado en MongoDB definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dfa90",
   "metadata": {},
   "source": [
    "## üîü Proceso Completo de Web Scraping\n",
    "\n",
    "Integrar todas las funciones en un flujo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e798688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Proceso completo definido\n"
     ]
    }
   ],
   "source": [
    "def proceso_scraping_completo(url, collection=None):\n",
    "    \"\"\"\n",
    "    Proceso completo de web scraping √©tico\n",
    "    \n",
    "    Args:\n",
    "        url: URL a scrapear\n",
    "        collection: Colecci√≥n de MongoDB (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con resultados\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéì TALLER BIG DATA - WEB SCRAPING √âTICO A ADRES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Scraping √©tico\n",
    "    print(f\"\\n1Ô∏è‚É£ Extrayendo contenido de: {url}\")\n",
    "    soup = scraping_etico(url)\n",
    "    \n",
    "    if soup is None:\n",
    "        print(\"‚ùå No se pudo extraer contenido\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Analizar contenido\n",
    "    print(\"\\n2Ô∏è‚É£ Analizando contenido...\")\n",
    "    analisis = analizar_contenido(soup, url)\n",
    "    \n",
    "    print(f\"   üìã T√≠tulo: {analisis['titulo'][:80]}...\")\n",
    "    print(f\"   üìù Longitud texto: {analisis['longitud_texto']} caracteres\")\n",
    "    print(f\"   üîë Palabras clave: {list(analisis['palabras_clave'].keys())}\")\n",
    "    print(f\"   üìÑ Resoluciones: {len(analisis['resoluciones_encontradas'])}\")\n",
    "    \n",
    "    # 3. Extraer enlaces\n",
    "    print(\"\\n3Ô∏è‚É£ Extrayendo enlaces a documentos...\")\n",
    "    documentos = extraer_enlaces_documentos(soup, url)\n",
    "    \n",
    "    # 4. Guardar en MongoDB (si est√° configurado)\n",
    "    if collection is not None:\n",
    "        print(\"\\n4Ô∏è‚É£ Guardando en MongoDB...\")\n",
    "        \n",
    "        # Crear documento completo\n",
    "        documento_completo = {\n",
    "            **analisis,\n",
    "            'documentos_relacionados': documentos,\n",
    "            'metadata': {\n",
    "                'fuente': 'ADRES',\n",
    "                'tipo_extraccion': 'web_scraping_etico',\n",
    "                'taller': 'Big Data 2025'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        doc_id = guardar_en_mongodb(collection, documento_completo)\n",
    "        \n",
    "        if doc_id:\n",
    "            print(f\"   üíæ ID MongoDB: {doc_id}\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è MongoDB no configurado - datos no guardados\")\n",
    "    \n",
    "    # 5. Resumen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PROCESO COMPLETADO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Documentos encontrados: {len(documentos)}\")\n",
    "    print(f\"üîë Palabras clave ADRES: {len(analisis['palabras_clave'])}\")\n",
    "    print(f\"‚úÖ Relevancia ADRES: {'S√ç' if analisis['relevancia_adres'] else 'NO'}\")\n",
    "    \n",
    "    return {\n",
    "        'analisis': analisis,\n",
    "        'documentos': documentos,\n",
    "        'exito': True\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Proceso completo definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491b606",
   "metadata": {},
   "source": [
    "## üöÄ Ejemplo de Uso\n",
    "\n",
    "Ejecutar web scraping a ADRES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Extraer datos de ADRES\n",
    "URL_ADRES = \"https://www.adres.gov.co/\"\n",
    "\n",
    "# Opci√≥n 1: Sin MongoDB (solo extracci√≥n)\n",
    "resultados = proceso_scraping_completo(URL_ADRES, collection=None)\n",
    "\n",
    "# Opci√≥n 2: Con MongoDB (descomentar y configurar)\n",
    "# MONGODB_URI = \"mongodb+srv://usuario:password@cluster.mongodb.net/\"\n",
    "# client, db, collection = conectar_mongodb(MONGODB_URI)\n",
    "# resultados = proceso_scraping_completo(URL_ADRES, collection)\n",
    "\n",
    "# Mostrar resultados\n",
    "if resultados:\n",
    "    print(f\"\\n‚úÖ Extracci√≥n completada:\")\n",
    "    print(f\"   üìä Documentos encontrados: {len(resultados['documentos'])}\")\n",
    "    print(f\"   üîë Palabras clave: {len(resultados['analisis']['palabras_clave'])}\")\n",
    "    print(f\"   üìù Caracteres analizados: {resultados['analisis']['longitud_texto']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56157c2e",
   "metadata": {},
   "source": [
    "## üîç Exploraci√≥n √âtica Multi-p√°gina\n",
    "\n",
    "Para encontrar m√°s PDFs sin causar da√±os, vamos a explorar m√∫ltiples p√°ginas relevantes de ADRES manteniendo los principios √©ticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b711fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n extraer_enlaces_internos() definida\n"
     ]
    }
   ],
   "source": [
    "def extraer_enlaces_internos(soup, url_base):\n",
    "    \"\"\"\n",
    "    Extrae enlaces internos de ADRES para explorar m√°s p√°ginas\n",
    "    \n",
    "    Args:\n",
    "        soup: Objeto BeautifulSoup\n",
    "        url_base: URL base del sitio\n",
    "    \n",
    "    Returns:\n",
    "        Lista de URLs internas a explorar\n",
    "    \"\"\"\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "    \n",
    "    enlaces_internos = set()\n",
    "    dominio_base = urlparse(url_base).netloc\n",
    "    \n",
    "    # Buscar todos los enlaces\n",
    "    for enlace in soup.find_all('a', href=True):\n",
    "        url_completa = urljoin(url_base, enlace['href'])\n",
    "        parsed = urlparse(url_completa)\n",
    "        \n",
    "        # Solo enlaces del mismo dominio\n",
    "        if parsed.netloc == dominio_base:\n",
    "            # Filtrar URLs relevantes (normograma, gu√≠as, resoluciones)\n",
    "            if any(palabra in url_completa.lower() for palabra in \n",
    "                   ['normograma', 'guia', 'resolucion', 'documento', 'compilacion', 'aprende']):\n",
    "                enlaces_internos.add(url_completa)\n",
    "    \n",
    "    return list(enlaces_internos)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n extraer_enlaces_internos() definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1bd4117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n scraping_multipagina_etico() definida\n"
     ]
    }
   ],
   "source": [
    "def scraping_multipagina_etico(url_inicial, max_paginas=5, delay=5):\n",
    "    \"\"\"\n",
    "    Explora m√∫ltiples p√°ginas de ADRES de forma √©tica buscando PDFs\n",
    "    \n",
    "    Args:\n",
    "        url_inicial: URL de inicio\n",
    "        max_paginas: M√°ximo de p√°ginas a explorar (default 5 para ser conservador)\n",
    "        delay: Segundos entre peticiones (m√≠nimo 5 para no sobrecargar)\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con todos los PDFs encontrados\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç EXPLORACI√ìN MULTI-P√ÅGINA √âTICA DE ADRES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚öôÔ∏è Configuraci√≥n:\")\n",
    "    print(f\"   ‚Ä¢ P√°ginas m√°ximas: {max_paginas}\")\n",
    "    print(f\"   ‚Ä¢ Delay entre peticiones: {delay} segundos\")\n",
    "    print(f\"   ‚Ä¢ Respeto robots.txt: S√ç\")\n",
    "    \n",
    "    # Verificar robots.txt\n",
    "    print(f\"\\n1Ô∏è‚É£ Verificando robots.txt...\")\n",
    "    if not verificar_robots_txt(url_inicial):\n",
    "        print(\"‚ùå robots.txt no permite scraping. Deteniendo.\")\n",
    "        return None\n",
    "    \n",
    "    todos_los_pdfs = {}\n",
    "    paginas_exploradas = set()\n",
    "    paginas_por_explorar = [url_inicial]\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ Iniciando exploraci√≥n...\")\n",
    "    \n",
    "    while paginas_por_explorar and len(paginas_exploradas) < max_paginas:\n",
    "        url_actual = paginas_por_explorar.pop(0)\n",
    "        \n",
    "        # Evitar repetir p√°ginas\n",
    "        if url_actual in paginas_exploradas:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n   üìÑ Explorando: {url_actual}\")\n",
    "        \n",
    "        # Delay √©tico\n",
    "        if len(paginas_exploradas) > 0:\n",
    "            print(f\"      ‚è≥ Esperando {delay} segundos...\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        # Scraping de la p√°gina\n",
    "        soup = scraping_etico(url_actual, delay=0)  # Ya esperamos arriba\n",
    "        \n",
    "        if soup is None:\n",
    "            print(f\"      ‚ö†Ô∏è No se pudo acceder\")\n",
    "            paginas_exploradas.add(url_actual)\n",
    "            continue\n",
    "        \n",
    "        # Extraer PDFs de esta p√°gina\n",
    "        pdfs_encontrados = extraer_enlaces_documentos(soup, url_actual)\n",
    "        \n",
    "        if pdfs_encontrados:\n",
    "            print(f\"      ‚úÖ {len(pdfs_encontrados)} PDF(s) encontrado(s)\")\n",
    "            for pdf in pdfs_encontrados:\n",
    "                todos_los_pdfs[pdf['url']] = pdf\n",
    "        else:\n",
    "            print(f\"      ‚ÑπÔ∏è No se encontraron PDFs\")\n",
    "        \n",
    "        # Extraer enlaces internos para explorar m√°s\n",
    "        if len(paginas_exploradas) < max_paginas - 1:\n",
    "            enlaces_internos = extraer_enlaces_internos(soup, url_actual)\n",
    "            \n",
    "            # Agregar nuevos enlaces (que no est√©n ya explorados)\n",
    "            for enlace in enlaces_internos[:3]:  # M√°ximo 3 enlaces por p√°gina\n",
    "                if enlace not in paginas_exploradas and enlace not in paginas_por_explorar:\n",
    "                    paginas_por_explorar.append(enlace)\n",
    "        \n",
    "        paginas_exploradas.add(url_actual)\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ EXPLORACI√ìN COMPLETADA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüìä Estad√≠sticas:\")\n",
    "    print(f\"   ‚Ä¢ P√°ginas exploradas: {len(paginas_exploradas)}\")\n",
    "    print(f\"   ‚Ä¢ PDFs √∫nicos encontrados: {len(todos_los_pdfs)}\")\n",
    "    print(f\"   ‚Ä¢ Tiempo total estimado: {len(paginas_exploradas) * delay} segundos\")\n",
    "    \n",
    "    if todos_los_pdfs:\n",
    "        print(f\"\\nüìÑ PDFs encontrados:\")\n",
    "        for i, (url_pdf, info) in enumerate(todos_los_pdfs.items(), 1):\n",
    "            print(f\"   {i}. {info.get('titulo', 'Sin t√≠tulo')[:60]}\")\n",
    "            print(f\"      URL: {url_pdf}\")\n",
    "    \n",
    "    return {\n",
    "        'pdfs': list(todos_los_pdfs.values()),\n",
    "        'total_pdfs': len(todos_los_pdfs),\n",
    "        'paginas_exploradas': list(paginas_exploradas),\n",
    "        'total_paginas': len(paginas_exploradas)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n scraping_multipagina_etico() definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3ca6f",
   "metadata": {},
   "source": [
    "## üöÄ Ejemplo de Uso - Exploraci√≥n Ampliada\n",
    "\n",
    "Ahora vamos a explorar m√∫ltiples p√°ginas de ADRES para encontrar m√°s PDFs de manera √©tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fb771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Exploraci√≥n multi-p√°gina disponible.\n",
      "üìù Para ejecutar, descomenta las siguientes l√≠neas:\n",
      "\n",
      "# resultados_multipagina = scraping_multipagina_etico(\n",
      "#     url_inicial='https://www.adres.gov.co/',\n",
      "#     max_paginas=5,  # Explorar m√°ximo 5 p√°ginas\n",
      "#     delay=5         # 5 segundos entre peticiones\n",
      "# )\n",
      "\n",
      "üí° Alternativamente, explorar URLs espec√≠ficas:\n",
      "\n",
      "# for url in URLS_ADRES_DOCUMENTOS:\n",
      "#     print(f'\\nüîç Analizando: {url}')\n",
      "#     soup = scraping_etico(url, delay=5)\n",
      "#     if soup:\n",
      "#         pdfs = extraer_enlaces_documentos(soup, url)\n",
      "#         print(f'   üìÑ PDFs encontrados: {len(pdfs)}')\n",
      "#         for pdf in pdfs:\n",
      "#             print(f'      ‚Ä¢ {pdf[\"url\"]}')\n"
     ]
    }
   ],
   "source": [
    "# URLs espec√≠ficas de ADRES con m√°s probabilidad de tener PDFs\n",
    "URLS_ADRES_DOCUMENTOS = [\n",
    "    \"https://www.adres.gov.co/\",  # P√°gina principal\n",
    "    \"https://normograma.adres.gov.co/compilacion/aprende_adres_guias.html\",  # Gu√≠as\n",
    "]\n",
    "\n",
    "# Ejecutar exploraci√≥n multi-p√°gina (COMENTAR SI NO DESEA EJECUTAR)\n",
    "# ADVERTENCIA: Esto tomar√° varios minutos debido a los delays √©ticos\n",
    "\n",
    "print(\"‚ÑπÔ∏è Exploraci√≥n multi-p√°gina disponible.\")\n",
    "print(\"üìù Para ejecutar, descomenta las siguientes l√≠neas:\")\n",
    "print()\n",
    "print(\"# resultados_multipagina = scraping_multipagina_etico(\")\n",
    "print(\"#     url_inicial='https://www.adres.gov.co/',\")\n",
    "print(\"#     max_paginas=5,  # Explorar m√°ximo 5 p√°ginas\")\n",
    "print(\"#     delay=5         # 5 segundos entre peticiones\")\n",
    "print(\"# )\")\n",
    "print()\n",
    "print(\"üí° Alternativamente, explorar URLs espec√≠ficas:\")\n",
    "print()\n",
    "print(\"# for url in URLS_ADRES_DOCUMENTOS:\")\n",
    "print(\"#     print(f'\\\\nüîç Analizando: {url}')\")\n",
    "print(\"#     soup = scraping_etico(url, delay=5)\")\n",
    "print(\"#     if soup:\")\n",
    "print(\"#         pdfs = extraer_enlaces_documentos(soup, url)\")\n",
    "print(\"#         print(f'   üìÑ PDFs encontrados: {len(pdfs)}')\")\n",
    "print(\"#         for pdf in pdfs:\")\n",
    "print(\"#             print(f'      ‚Ä¢ {pdf[\\\"url\\\"]}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d66f54",
   "metadata": {},
   "source": [
    "## ‚úÖ Prueba R√°pida - URLs Espec√≠ficas\n",
    "\n",
    "Vamos a probar con URLs espec√≠ficas conocidas de ADRES para encontrar m√°s PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c103fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Explorando URLs espec√≠ficas de ADRES...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìÑ [1/2] Analizando: https://www.adres.gov.co/\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# URLs espec√≠ficas de ADRES con documentos\n",
    "urls_adres_documentos = [\n",
    "    \"https://www.adres.gov.co/\",\n",
    "    \"https://normograma.adres.gov.co/compilacion/aprende_adres_guias.html\",\n",
    "]\n",
    "\n",
    "print(\"üîç Explorando URLs espec√≠ficas de ADRES...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "todos_pdfs_encontrados = []\n",
    "\n",
    "for i, url in enumerate(urls_adres_documentos, 1):\n",
    "    print(f\"\\nüìÑ [{i}/{len(urls_adres_documentos)}] Analizando: {url}\")\n",
    "    \n",
    "    # Delay √©tico adicional entre p√°ginas diferentes\n",
    "    if i > 1:\n",
    "        print(f\"   ‚è≥ Esperando 3 segundos adicionales entre p√°ginas...\")\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Realizar scraping (la funci√≥n ya tiene delay interno de 3s)\n",
    "    soup = scraping_etico(url)\n",
    "    \n",
    "    if soup:\n",
    "        # Extraer PDFs\n",
    "        pdfs = extraer_enlaces_documentos(soup, url)\n",
    "        \n",
    "        if pdfs:\n",
    "            print(f\"   ‚úÖ {len(pdfs)} PDF(s) encontrado(s):\")\n",
    "            for pdf in pdfs:\n",
    "                url_corta = pdf['url'][:70] + '...' if len(pdf['url']) > 70 else pdf['url']\n",
    "                print(f\"      ‚Ä¢ {url_corta}\")\n",
    "                todos_pdfs_encontrados.append(pdf)\n",
    "        else:\n",
    "            print(f\"   ‚ÑπÔ∏è No se encontraron PDFs en esta p√°gina\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå No se pudo acceder a la p√°gina\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EXPLORACI√ìN COMPLETADA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"   ‚Ä¢ URLs exploradas: {len(urls_adres_documentos)}\")\n",
    "print(f\"   ‚Ä¢ Total PDFs encontrados: {len(todos_pdfs_encontrados)}\")\n",
    "\n",
    "# Eliminar duplicados por URL\n",
    "pdfs_unicos = {pdf['url']: pdf for pdf in todos_pdfs_encontrados}\n",
    "print(f\"   ‚Ä¢ PDFs √∫nicos: {len(pdfs_unicos)}\")\n",
    "print(f\"   ‚Ä¢ Tiempo total: ~{len(urls_adres_documentos) * 3 + (len(urls_adres_documentos)-1) * 3} segundos\")\n",
    "\n",
    "if pdfs_unicos:\n",
    "    print(f\"\\nüìÑ Lista de PDFs √∫nicos encontrados:\")\n",
    "    for i, pdf in enumerate(pdfs_unicos.values(), 1):\n",
    "        print(f\"   {i}. {pdf['url']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
