{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92df22d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéì Universidad:** Universidad Central de Colombia\n",
    "\n",
    "**üë®‚Äçüíª Autor:** Efren Bohorquez Vargas\n",
    "\n",
    "**üìö Curso:** Big Data\n",
    "\n",
    "**üë®‚Äçüè´ Presentado a:** Luis Fernando Castellanos\n",
    "\n",
    "**üìÖ Fecha:** Octubre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b645d",
   "metadata": {},
   "source": [
    "# üéì Taller Big Data: Web Scraping √âtico a ADRES\n",
    "\n",
    "**ADRES**: Administradora de los Recursos del Sistema General de Seguridad Social en Salud\n",
    "\n",
    "## üìã Objetivos del Taller\n",
    "\n",
    "1. Comprender los principios del **web scraping √©tico**\n",
    "2. Extraer documentos oficiales de ADRES\n",
    "3. Almacenar datos estructurados en **MongoDB Atlas**\n",
    "4. Analizar contenido de documentos legales\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881e26f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Instalaci√≥n de Dependencias\n",
    "\n",
    "Ejecutar solo si las librer√≠as no est√°n instaladas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar si necesitas instalar las dependencias\n",
    "# !pip install requests beautifulsoup4 pymongo urllib3 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b95a38",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13318964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb51b6",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuraci√≥n √âtica del Scraper\n",
    "\n",
    "### Principios √âticos:\n",
    "- ‚úÖ Respetar `robots.txt`\n",
    "- ‚úÖ Implementar delays entre requests (2+ segundos)\n",
    "- ‚úÖ User-Agent identificado\n",
    "- ‚úÖ Solo contenido p√∫blico\n",
    "- ‚úÖ Prop√≥sito educativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dc374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuraci√≥n √©tica establecida\n",
      "   ‚Ä¢ Delay: 3.0s\n",
      "   ‚Ä¢ User-Agent: TallerBigData-WebScraping/1.0 (Educativo; Python/requests)\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n √©tica del scraper\n",
    "CONFIGURACION_ETICA = {\n",
    "    'delay_entre_requests': 3.0,  # 3 segundos entre peticiones\n",
    "    'max_reintentos': 3,\n",
    "    'timeout': 30,\n",
    "    'user_agent': 'TallerBigData-WebScraping/1.0 (Educativo; Python/requests)',\n",
    "    'headers': {\n",
    "        'User-Agent': 'TallerBigData-WebScraping/1.0 (Educativo; Python/requests)',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'es-CO,es;q=0.9',\n",
    "        'Purpose': 'Educational Research - Big Data Workshop'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuraci√≥n √©tica establecida\")\n",
    "print(f\"   ‚Ä¢ Delay: {CONFIGURACION_ETICA['delay_entre_requests']}s\")\n",
    "print(f\"   ‚Ä¢ User-Agent: {CONFIGURACION_ETICA['user_agent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277547ee",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verificar robots.txt\n",
    "\n",
    "Siempre verificar qu√© permite el sitio web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b92ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No se encontr√≥ robots.txt (permitido por defecto)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verificar_robots_txt(url_base):\n",
    "    \"\"\"Verificar y mostrar robots.txt del sitio\"\"\"\n",
    "    robots_url = urljoin(url_base, '/robots.txt')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"ü§ñ robots.txt encontrado:\")\n",
    "            print(\"‚îÄ\" * 50)\n",
    "            print(response.text[:500])  # Primeras 500 caracteres\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se encontr√≥ robots.txt (permitido por defecto)\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error verificando robots.txt: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verificar robots.txt de ADRES\n",
    "URL_BASE_ADRES = \"https://www.adres.gov.co\"\n",
    "verificar_robots_txt(URL_BASE_ADRES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d64083",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Funci√≥n de Scraping √âtico\n",
    "\n",
    "Implementaci√≥n con delays y manejo de errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aea5378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de scraping √©tico definida\n"
     ]
    }
   ],
   "source": [
    "def scraping_etico(url, config=CONFIGURACION_ETICA):\n",
    "    \"\"\"\n",
    "    Realizar scraping √©tico con delays y reintentos\n",
    "    \n",
    "    Args:\n",
    "        url: URL a scrapear\n",
    "        config: Configuraci√≥n √©tica\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object o None si falla\n",
    "    \"\"\"\n",
    "    for intento in range(config['max_reintentos']):\n",
    "        try:\n",
    "            # Delay √©tico antes de hacer request\n",
    "            if intento > 0:\n",
    "                print(f\"   ‚è≥ Reintento {intento + 1}/{config['max_reintentos']}...\")\n",
    "            \n",
    "            time.sleep(config['delay_entre_requests'])\n",
    "            \n",
    "            # Realizar request\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers=config['headers'],\n",
    "                timeout=config['timeout']\n",
    "            )\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parsear HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            print(f\"‚úÖ Contenido extra√≠do exitosamente de: {url}\")\n",
    "            return soup\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error en intento {intento + 1}: {e}\")\n",
    "            if intento == config['max_reintentos'] - 1:\n",
    "                print(f\"‚ùå Fall√≥ despu√©s de {config['max_reintentos']} intentos\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de scraping √©tico definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7f0e6",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Extraer Enlaces a Documentos\n",
    "\n",
    "Buscar enlaces a PDFs y documentos oficiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38432f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de extracci√≥n de enlaces definida\n"
     ]
    }
   ],
   "source": [
    "def extraer_enlaces_documentos(soup, url_base):\n",
    "    \"\"\"\n",
    "    Extraer enlaces a documentos (PDFs, resoluciones, etc.)\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        url_base: URL base para construir URLs absolutas\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con informaci√≥n de documentos\n",
    "    \"\"\"\n",
    "    documentos = []\n",
    "    \n",
    "    # Buscar todos los enlaces\n",
    "    enlaces = soup.find_all('a', href=True)\n",
    "    \n",
    "    for enlace in enlaces:\n",
    "        href = enlace['href']\n",
    "        texto = enlace.get_text(strip=True)\n",
    "        \n",
    "        # Filtrar enlaces a documentos\n",
    "        if any(ext in href.lower() for ext in ['.pdf', 'resolucion', 'documento']):\n",
    "            url_completa = urljoin(url_base, href)\n",
    "            \n",
    "            documentos.append({\n",
    "                'url': url_completa,\n",
    "                'texto': texto,\n",
    "                'tipo': 'pdf' if '.pdf' in href.lower() else 'documento',\n",
    "                'fecha_extraccion': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    print(f\"üìÑ Encontrados {len(documentos)} enlaces a documentos\")\n",
    "    return documentos\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n de enlaces definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e0b42",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Analizar Contenido de P√°gina\n",
    "\n",
    "Extraer informaci√≥n estructurada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e5fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de an√°lisis de contenido definida\n"
     ]
    }
   ],
   "source": [
    "def analizar_contenido(soup, url):\n",
    "    \"\"\"\n",
    "    Analizar y extraer contenido estructurado\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        url: URL de origen\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con an√°lisis del contenido\n",
    "    \"\"\"\n",
    "    # Extraer t√≠tulo\n",
    "    titulo = soup.find('title')\n",
    "    titulo_texto = titulo.get_text(strip=True) if titulo else 'Sin t√≠tulo'\n",
    "    \n",
    "    # Extraer texto principal\n",
    "    texto_completo = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Palabras clave relacionadas con ADRES\n",
    "    palabras_clave = ['adres', 'salud', 'seguridad social', 'resoluci√≥n', \n",
    "                      'administradora', 'recursos', 'eps', 'ips']\n",
    "    \n",
    "    palabras_encontradas = {}\n",
    "    for palabra in palabras_clave:\n",
    "        cuenta = texto_completo.lower().count(palabra.lower())\n",
    "        if cuenta > 0:\n",
    "            palabras_encontradas[palabra] = cuenta\n",
    "    \n",
    "    # Extraer n√∫meros de resoluci√≥n (ejemplo: Resoluci√≥n 2876 de 2013)\n",
    "    resoluciones = re.findall(r'resoluci√≥n\\s+(\\d+)\\s+de\\s+(\\d{4})', \n",
    "                              texto_completo, re.IGNORECASE)\n",
    "    \n",
    "    analisis = {\n",
    "        'url': url,\n",
    "        'titulo': titulo_texto,\n",
    "        'longitud_texto': len(texto_completo),\n",
    "        'palabras_clave': palabras_encontradas,\n",
    "        'resoluciones_encontradas': resoluciones,\n",
    "        'fecha_analisis': datetime.now().isoformat(),\n",
    "        'relevancia_adres': len(palabras_encontradas) > 0\n",
    "    }\n",
    "    \n",
    "    return analisis\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de an√°lisis de contenido definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315181fc",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Conexi√≥n a MongoDB Atlas\n",
    "\n",
    "**Nota**: Reemplazar la cadena de conexi√≥n con tu propia configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8876f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de conexi√≥n a MongoDB definida\n"
     ]
    }
   ],
   "source": [
    "def conectar_mongodb(connection_string=None):\n",
    "    \"\"\"\n",
    "    Conectar a MongoDB Atlas\n",
    "    \n",
    "    Args:\n",
    "        connection_string: String de conexi√≥n de MongoDB Atlas\n",
    "    \n",
    "    Returns:\n",
    "        Tupla (client, database, collection)\n",
    "    \"\"\"\n",
    "    # IMPORTANTE: Reemplazar con tu propia connection string\n",
    "    if connection_string is None:\n",
    "        print(\"‚ö†Ô∏è Configurar connection_string de MongoDB Atlas\")\n",
    "        print(\"   Ejemplo: mongodb+srv://usuario:password@cluster.mongodb.net/\")\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        client = MongoClient(connection_string, serverSelectionTimeoutMS=5000)\n",
    "        \n",
    "        # Verificar conexi√≥n\n",
    "        client.admin.command('ping')\n",
    "        \n",
    "        db = client['taller_bigdata_adres']\n",
    "        collection = db['documentos_adres']\n",
    "        \n",
    "        print(\"‚úÖ Conectado a MongoDB Atlas\")\n",
    "        print(f\"   üìä Base de datos: {db.name}\")\n",
    "        print(f\"   üìÅ Colecci√≥n: {collection.name}\")\n",
    "        \n",
    "        return client, db, collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error conectando a MongoDB: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Descomentar y configurar tu connection string\n",
    "# MONGODB_URI = \"mongodb+srv://usuario:password@cluster.mongodb.net/\"\n",
    "# client, db, collection = conectar_mongodb(MONGODB_URI)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de conexi√≥n a MongoDB definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e80918",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Guardar en MongoDB\n",
    "\n",
    "Almacenar documentos extra√≠dos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271f9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de guardado en MongoDB definida\n"
     ]
    }
   ],
   "source": [
    "def guardar_en_mongodb(collection, documento):\n",
    "    \"\"\"\n",
    "    Guardar documento en MongoDB\n",
    "    \n",
    "    Args:\n",
    "        collection: Colecci√≥n de MongoDB\n",
    "        documento: Diccionario con datos a guardar\n",
    "    \n",
    "    Returns:\n",
    "        ID del documento insertado o None\n",
    "    \"\"\"\n",
    "    if collection is None:\n",
    "        print(\"‚ö†Ô∏è Colecci√≥n no configurada\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Verificar si ya existe (evitar duplicados)\n",
    "        existe = collection.find_one({'url': documento.get('url')})\n",
    "        \n",
    "        if existe:\n",
    "            print(f\"‚ÑπÔ∏è Documento ya existe: {documento.get('titulo', 'Sin t√≠tulo')}\")\n",
    "            return existe['_id']\n",
    "        \n",
    "        # Insertar nuevo documento\n",
    "        resultado = collection.insert_one(documento)\n",
    "        print(f\"‚úÖ Documento guardado: {documento.get('titulo', 'Sin t√≠tulo')}\")\n",
    "        \n",
    "        return resultado.inserted_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error guardando documento: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de guardado en MongoDB definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dfa90",
   "metadata": {},
   "source": [
    "## üîü Proceso Completo de Web Scraping\n",
    "\n",
    "Integrar todas las funciones en un flujo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e798688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Proceso completo definido\n"
     ]
    }
   ],
   "source": [
    "def proceso_scraping_completo(url, collection=None):\n",
    "    \"\"\"\n",
    "    Proceso completo de web scraping √©tico\n",
    "    \n",
    "    Args:\n",
    "        url: URL a scrapear\n",
    "        collection: Colecci√≥n de MongoDB (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con resultados\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéì TALLER BIG DATA - WEB SCRAPING √âTICO A ADRES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Scraping √©tico\n",
    "    print(f\"\\n1Ô∏è‚É£ Extrayendo contenido de: {url}\")\n",
    "    soup = scraping_etico(url)\n",
    "    \n",
    "    if soup is None:\n",
    "        print(\"‚ùå No se pudo extraer contenido\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Analizar contenido\n",
    "    print(\"\\n2Ô∏è‚É£ Analizando contenido...\")\n",
    "    analisis = analizar_contenido(soup, url)\n",
    "    \n",
    "    print(f\"   üìã T√≠tulo: {analisis['titulo'][:80]}...\")\n",
    "    print(f\"   üìù Longitud texto: {analisis['longitud_texto']} caracteres\")\n",
    "    print(f\"   üîë Palabras clave: {list(analisis['palabras_clave'].keys())}\")\n",
    "    print(f\"   üìÑ Resoluciones: {len(analisis['resoluciones_encontradas'])}\")\n",
    "    \n",
    "    # 3. Extraer enlaces\n",
    "    print(\"\\n3Ô∏è‚É£ Extrayendo enlaces a documentos...\")\n",
    "    documentos = extraer_enlaces_documentos(soup, url)\n",
    "    \n",
    "    # 4. Guardar en MongoDB (si est√° configurado)\n",
    "    if collection is not None:\n",
    "        print(\"\\n4Ô∏è‚É£ Guardando en MongoDB...\")\n",
    "        \n",
    "        # Crear documento completo\n",
    "        documento_completo = {\n",
    "            **analisis,\n",
    "            'documentos_relacionados': documentos,\n",
    "            'metadata': {\n",
    "                'fuente': 'ADRES',\n",
    "                'tipo_extraccion': 'web_scraping_etico',\n",
    "                'taller': 'Big Data 2025'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        doc_id = guardar_en_mongodb(collection, documento_completo)\n",
    "        \n",
    "        if doc_id:\n",
    "            print(f\"   üíæ ID MongoDB: {doc_id}\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è MongoDB no configurado - datos no guardados\")\n",
    "    \n",
    "    # 5. Resumen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PROCESO COMPLETADO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Documentos encontrados: {len(documentos)}\")\n",
    "    print(f\"üîë Palabras clave ADRES: {len(analisis['palabras_clave'])}\")\n",
    "    print(f\"‚úÖ Relevancia ADRES: {'S√ç' if analisis['relevancia_adres'] else 'NO'}\")\n",
    "    \n",
    "    return {\n",
    "        'analisis': analisis,\n",
    "        'documentos': documentos,\n",
    "        'exito': True\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Proceso completo definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491b606",
   "metadata": {},
   "source": [
    "## üöÄ Ejemplo de Uso\n",
    "\n",
    "Ejecutar web scraping a ADRES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Extraer datos de ADRES\n",
    "URL_ADRES = \"https://www.adres.gov.co/\"\n",
    "\n",
    "# Opci√≥n 1: Sin MongoDB (solo extracci√≥n)\n",
    "resultados = proceso_scraping_completo(URL_ADRES, collection=None)\n",
    "\n",
    "# Opci√≥n 2: Con MongoDB (descomentar y configurar)\n",
    "# MONGODB_URI = \"mongodb+srv://usuario:password@cluster.mongodb.net/\"\n",
    "# client, db, collection = conectar_mongodb(MONGODB_URI)\n",
    "# resultados = proceso_scraping_completo(URL_ADRES, collection)\n",
    "\n",
    "# Mostrar resultados\n",
    "if resultados:\n",
    "    print(f\"\\n‚úÖ Extracci√≥n completada:\")\n",
    "    print(f\"   üìä Documentos encontrados: {len(resultados['documentos'])}\")\n",
    "    print(f\"   üîë Palabras clave: {len(resultados['analisis']['palabras_clave'])}\")\n",
    "    print(f\"   üìù Caracteres analizados: {resultados['analisis']['longitud_texto']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
