# ğŸ•·ï¸ Web Scraping Ã‰tico ADRES - Taller Big Data# ADRES Scraper ğŸ‡¨ğŸ‡´



[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)

[![BeautifulSoup4](https://img.shields.io/badge/BeautifulSoup4-4.12+-green.svg)](https://www.crummy.com/software/BeautifulSoup/)[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)



**Proyecto educativo de Web Scraping Ã‰tico** aplicado al sitio web de ADRES (Administradora de Recursos del Sistema General de Seguridad Social en Salud).Web scraping Ã©tico para documentos oficiales de ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud) de Colombia.



## ğŸ‘¨â€ğŸ“ InformaciÃ³n AcadÃ©mica## ğŸ“‹ DescripciÃ³n



- **Universidad**: Universidad Central de ColombiaEste proyecto proporciona una soluciÃ³n completa para la descarga y almacenamiento Ã©tico de documentos PDF de ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud), la entidad colombiana encargada de administrar los recursos financieros del sistema de salud. Respeta las polÃ­ticas del sitio web e implementa buenas prÃ¡cticas de web scraping.

- **Curso**: Taller de Big Data

- **Autor**: Efren Bohorquez Vargas### CaracterÃ­sticas principales

- **Instructor**: Luis Fernando Castellanos

- **AÃ±o**: 2024-2025- âœ… **Scraping Ã‰tico**: Implementa delays apropiados y respeta el robots.txt

- ğŸ“„ **Descarga de PDFs**: Descarga automÃ¡tica de documentos oficiales

## ğŸ“‹ DescripciÃ³n- ğŸ—„ï¸ **Almacenamiento MongoDB**: Guarda documentos y metadatos en MongoDB Atlas

- ğŸ” **AnÃ¡lisis de Contenido**: Extrae informaciÃ³n relevante de documentos

Este proyecto implementa tÃ©cnicas de web scraping Ã©tico para extraer y analizar informaciÃ³n del portal de ADRES, respetando:- ğŸ“Š **Monitoreo Visual**: Interfaz para seguimiento de procesos

- ğŸ›¡ï¸ **ValidaciÃ³n Ã‰tica**: VerificaciÃ³n automÃ¡tica de cumplimiento normativo

- âœ… Robots.txt y polÃ­ticas del sitio

- âœ… Delays entre peticiones (3 segundos mÃ­nimo)## ğŸš€ InstalaciÃ³n RÃ¡pida

- âœ… User-Agent identificado

- âœ… LÃ­mites de tasa de solicitudes```bash

- âœ… Sin sobrecarga del servidor# Clonar el repositorio

git clone https://github.com/tu-usuario/adres-scraper.git

## ğŸ¯ CaracterÃ­sticascd adres-scraper



- **Scraping Ã‰tico**: ConfiguraciÃ³n profesional con principios Ã©ticos# Crear entorno virtual

- **AnÃ¡lisis de Contenido**: ExtracciÃ³n de keywords y metadatapython -m venv venv

- **DetecciÃ³n de PDFs**: IdentificaciÃ³n automÃ¡tica de documentossource venv/bin/activate  # En Windows: venv\Scripts\activate

- **MongoDB Atlas**: Almacenamiento estructurado opcional

- **Jupyter Notebook**: Tutorial interactivo completo# Instalar dependencias

pip install -r requirements.txt

## ğŸ“ Estructura del Proyecto```



```## ğŸ“– Uso BÃ¡sico

web_scraping_adres_taller/

â”‚```python

â”œâ”€â”€ ğŸ““ Taller_WebScraping_ADRES.ipynb    # Notebook educativo principalfrom adres_scraper import ADREScraper, MongoDBManager, PDFDownloader, Config

â”œâ”€â”€ ğŸ“„ Taller_WebScraping_ADRES.html     # ExportaciÃ³n HTML

â”œâ”€â”€ ğŸ“„ Taller_WebScraping_ADRES.pdf      # DocumentaciÃ³n PDF# Cargar configuraciÃ³n

â”‚config = Config.from_file('config/config_mongodb_atlas.json')

â”œâ”€â”€ src/                                  # MÃ³dulos core

â”‚   â”œâ”€â”€ web_scraper_adres.py             # Scraper principal# Inicializar componentes

â”‚   â”œâ”€â”€ validador_etico.py               # ValidaciÃ³n Ã©ticascraper = ADREScraper(config)

â”‚   â”œâ”€â”€ mongodb_manager.py               # GestiÃ³n MongoDBdb_manager = MongoDBManager(config)

â”‚   â””â”€â”€ analizador_contenido.py          # AnÃ¡lisis de contenidopdf_downloader = PDFDownloader(config)

â”‚

â”œâ”€â”€ config/                               # Configuraciones# Realizar scraping

â”œâ”€â”€ data/                                 # Datos extraÃ­dosurl = "https://www.adres.gov.co/biblioteca-y-publicaciones/resoluciones"

â”œâ”€â”€ docs/                                 # DocumentaciÃ³n adicionalresultado = scraper.scrape_document(url)

â”œâ”€â”€ logs/                                 # Logs del sistema

â”œâ”€â”€ tests/                                # Tests unitariosif resultado['exito']:

â”œâ”€â”€ archive_tests/                        # Scripts archivados    # Descargar PDFs encontrados

â”‚    pdfs_descargados = pdf_downloader.download_pdfs_from_page(url)

â”œâ”€â”€ requirements.txt                      # Dependencias Python    

â”œâ”€â”€ setup.py                              # InstalaciÃ³n del paquete    # Almacenar en MongoDB

â”œâ”€â”€ .gitignore                           # Archivos ignorados por Git    for pdf_info in pdfs_descargados:

â””â”€â”€ README.md                            # Este archivo        db_manager.store_pdf_file(

            pdf_content=pdf_info['contenido'],

```            filename=pdf_info['nombre_archivo'],

            metadata=pdf_info['metadata']

## ğŸš€ InstalaciÃ³n RÃ¡pida        )

```

### 1. Clonar el repositorio

## ğŸ—ï¸ Arquitectura

```bash

git clone https://github.com/efrenbohorquez/ADRES-Scraper-v2.0.git```

cd ADRES-Scraper-v2.0adres_scraper/

```â”œâ”€â”€ core/                   # NÃºcleo del sistema

â”œâ”€â”€ mongodb/               # GestiÃ³n de base de datos  

### 2. Crear entorno virtualâ”œâ”€â”€ downloaders/           # Descargadores especializados

â”œâ”€â”€ config/                # ConfiguraciÃ³n del sistema

```bashâ””â”€â”€ utils/                 # Utilidades auxiliares

# Windows PowerShell```

python -m venv venv

.\venv\Scripts\Activate.ps1## âš™ï¸ ConfiguraciÃ³n



# Linux/MacCrear archivo de configuraciÃ³n:

python3 -m venv venv```json

source venv/bin/activate{

```  "mongodb": {

    "connection_string": "mongodb+srv://...",

### 3. Instalar dependencias    "database_name": "adres_scraper"

  },

```bash  "scraping": {

pip install -r requirements.txt    "delay_between_requests": 2.0,

```    "max_retries": 3,

    "timeout": 30

### 4. Ejecutar el Notebook  }

}

```bash```

jupyter notebook Taller_WebScraping_ADRES.ipynb

```## ğŸ¤ Ã‰tica y Cumplimiento



## ğŸ“¦ Dependencias Principales- **Respeto al robots.txt**: VerificaciÃ³n automÃ¡tica

- **Delays apropiados**: 2+ segundos entre requests

```- **User-Agent identificado**: Transparencia total

beautifulsoup4>=4.12.0   # Parsing HTML- **Solo contenido pÃºblico**: Sin acceso restringido

requests>=2.31.0          # Peticiones HTTP- **PropÃ³sito educativo**: Uso acadÃ©mico Ãºnicamente

lxml>=4.9.0              # Parser rÃ¡pido

pymongo>=4.5.0           # MongoDB (opcional)## ğŸ“ Licencia

jupyter>=1.0.0           # Notebooks interactivos

```MIT License - Ver [LICENSE](LICENSE) para mÃ¡s detalles.



## ğŸ’» Uso BÃ¡sico## ğŸ‘¥ Autores



### Desde Python**Taller Big Data 2025** - Desarrollo inicial



```python---

from src.web_scraper_adres import scraping_etico, analizar_contenido

**ğŸ‡¨ğŸ‡´ Hecho con â¤ï¸ para la comunidad de datos colombiana**
# URL objetivo
url = "https://www.adres.gov.co/"

# Realizar scraping Ã©tico
resultado = scraping_etico(url, delay=3)

if resultado["exito"]:
    # Analizar contenido
    analisis = analizar_contenido(
        resultado["contenido"],
        resultado["url_final"]
    )
    
    print(f"Caracteres extraÃ­dos: {analisis['longitud_texto']}")
    print(f"Keywords encontradas: {analisis['keywords_relevantes']}")
    print(f"PDFs detectados: {len(analisis['enlaces_pdfs'])}")
```

### Desde Jupyter Notebook

Abrir `Taller_WebScraping_ADRES.ipynb` y ejecutar todas las celdas secuencialmente.

## ğŸ”§ ConfiguraciÃ³n Ã‰tica

El proyecto incluye configuraciÃ³n Ã©tica predefinida:

```python
CONFIGURACION_ETICA = {
    "delay_entre_peticiones": 3,      # 3 segundos mÃ­nimo
    "timeout": 15,                     # Timeout de 15 segundos
    "max_reintentos": 3,               # MÃ¡ximo 3 reintentos
    "identificacion": "TallerBigData", # User-Agent identificado
    "respetar_robots_txt": True        # Verificar robots.txt
}
```

## ğŸ“Š Resultados del Scraping

El scraping de ADRES proporciona:

- **Texto extraÃ­do**: ~37,000+ caracteres
- **Keywords identificadas**: "salud", "afiliaciÃ³n", "prestadores", etc.
- **PDFs encontrados**: GuÃ­as, resoluciones, normativa
- **Metadata**: Fecha, URL, cÃ³digo de respuesta

## ğŸ§ª Tests

```bash
# Ejecutar todos los tests
pytest tests/

# Con cobertura
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“ˆ AnÃ¡lisis de Datos

Los datos extraÃ­dos se pueden analizar con:

- **Pandas**: AnÃ¡lisis estructurado
- **WordCloud**: Nube de palabras
- **Matplotlib/Seaborn**: Visualizaciones
- **NLTK**: Procesamiento de lenguaje natural

## ğŸ” MongoDB Atlas (Opcional)

Para almacenar los resultados en MongoDB:

1. Crear cuenta en [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)
2. Configurar `config/config_mongodb_atlas.json`
3. Ejecutar las funciones de conexiÃ³n en el notebook

## ğŸ“š Recursos Adicionales

- **ADRES**: https://www.adres.gov.co/
- **Normograma ADRES**: https://normograma.adres.gov.co/
- **BeautifulSoup Docs**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Ethical Web Scraping**: https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01

## ğŸ“ Principios Ã‰ticos Aplicados

1. **Respeto a robots.txt**: VerificaciÃ³n previa obligatoria
2. **IdentificaciÃ³n clara**: User-Agent con propÃ³sito educativo
3. **Rate limiting**: Delays configurados entre peticiones
4. **Uso responsable**: Solo extracciÃ³n de informaciÃ³n pÃºblica
5. **No sobrecarga**: LÃ­mites de peticiones y timeouts
6. **Transparencia**: CÃ³digo abierto y documentado

## ğŸ¤ Contribuciones

Este es un proyecto educativo. Para sugerencias:

1. Fork el repositorio
2. Crea una rama (`git checkout -b feature/mejora`)
3. Commit cambios (`git commit -m 'Agregar mejora'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## âš ï¸ Disclaimer

Este proyecto es **Ãºnicamente para fines educativos**. El web scraping debe realizarse de manera Ã©tica y respetando:

- TÃ©rminos de servicio del sitio web
- Leyes de propiedad intelectual
- PolÃ­ticas de privacidad de datos
- Normativa legal aplicable

**No utilizar para fines comerciales sin autorizaciÃ³n.**

## ğŸ“§ Contacto

**Efren Bohorquez Vargas**
- ğŸ“ Universidad Central de Colombia
- ğŸ“§ Email: [Tu correo institucional]
- ğŸ’¼ LinkedIn: [Tu perfil]
- ğŸ™ GitHub: [@efrenbohorquez](https://github.com/efrenbohorquez)

---

**Desarrollado con â¤ï¸ para el Taller de Big Data**

*Universidad Central de Colombia - 2024-2025*
